#+TITLE: Halpha-emitters
* Find  SPLUS Halpha Emitters
:PROPERTIES:
:ID:       
:END:

** Motivation
I want to know the population of H\alpha emitter in SPLUS DR3 and described what kind of objects they are.

*** Previously papers related

+ Drew paper.

#+BEGIN_SRC bibtex

@article{Drew:2005,
    author = {Drew, Janet E. and Greimel, R. and Irwin, M. J. and Aungwerojwit, A. and Barlow, M. J. and Corradi, R. L. M. and Drake, J. J. and Gänsicke, B. T. and Groot, P. and Hales, A. and Hopewell, E. C. and Irwin, J. and Knigge, C. and Leisy, P. and Lennon, D. J. and Mampaso, A. and Masheder, M. R. W. and Matsuura, M. and Morales-Rueda, L. and Morris, R. A. H. and Parker, Q. A. and Phillipps, S. and Rodriguez-Gil, P. and Roelofs, G. and Skillen, I. and Sokoloski, J. L. and Steeghs, D. and Unruh, Y. C. and Viironen, K. and Vink, J. S. and Walton, N. A. and Witham, A. and Wright, N. and Zijlstra, A. A. and Zurita, A.},
    title = "{The INT Photometric Hα Survey of the Northern Galactic Plane (IPHAS)}",
    journal = {Monthly Notices of the Royal Astronomical Society},
    volume = {362},
    number = {3},
    pages = {753-776},
    year = {2005},
    month = {09},
    abstract = "{The Isaac Newton Telescope (INT) Photometric Hα Survey of the Northern Galactic Plane 
                (IPHAS) is a 1800-deg2 CCD survey of the northern Milky Way spanning the latitude range -5^$\degree$ \\&lt; b \\&lt; + 5° 
                and reaching down to r′≃ 20 (10s). Representative observations and an assessment of point-source data from IPHAS, 
                now underway, are presented. The data obtained are Wide Field Camera images in the Hα narrow-band, and Sloan r′ 
                and i′ broad-band filters. We simulate IPHAS (r′- H$\alpha$, r′- i′) point-source colours using a spectrophotometric 
                library of stellar spectra and available filter transmission profiles: this defines the expected colour properties of (i) 
                solar metallicity stars, without Hα emission, and (ii) emission-line stars. Comparisons with observations of fields 
                in Aquila show that the simulations of normal star colours reproduce the observations well for all spectral types 
                earlier than M. A further comparison between colours synthesized from long-slit flux-calibrated spectra and IPHAS 
                photometry for six objects in a Taurus field confirms the reliability of the pipeline calibration. Spectroscopic 
                follow-up of a field in Cepheus shows that sources lying above the main stellar locus in the (r′− Hα, r′−i′) plane 
                are confirmed to be emission-line objects with very few failures. In this same field, examples of Hα deficit objects 
                (a white dwarf and a carbon star) are shown to be readily distinguished by their IPHAS colours. The role IPHAS can play 
                in studies of spatially resolved northern Galactic nebulae is discussed briefly and illustrated by a continuum-subtracted 
                mosaic image of Shajn 147 (a supernova remnant, 3° in diameter). The final catalogue of IPHAS point sources will 
                contain photometry on about 80 million objects. Used on its own, or in combination with near-infrared photometric 
                catalogues, IPHAS is a major resource for the study of stellar populations making up the disc of the Milky Way. 
                The eventual yield of new northern emission-line objects from IPHAS is likely to be an order of magnitude increase 
                on the number already known.}",
    issn = {0035-8711},
    doi = {10.1111/j.1365-2966.2005.09330.x},
    url = {https://doi.org/10.1111/j.1365-2966.2005.09330.x},
    eprint = {https://academic.oup.com/mnras/article-pdf/362/3/753/2986007/362-3-753.pdf},}

#+END_SRC

+ [[https://ui.adsabs.harvard.edu/abs/2006MNRAS.369..581W/abstract][Witham et el. (2006)]] report on the properties of 71 known cataclysmic variables (CVs) in photometric 
  H\alpha emission-line surveys.  

+ [[https://ui.adsabs.harvard.edu/abs/2008MNRAS.384.1277W/abstract][Witham et el. (2008)]] present a catalogue of point-sources H\alpha emission objects identified in IPHAS.

+ [[https://ui.adsabs.harvard.edu/abs/2013MNRAS.428.2207S/abstract][Scaringi et al. (2013]]) present the first results of an ongoing spectroscopic follow-up programme
  of blue-H\alpha-excess sources within the Kepler field of view in order to identify new cataclysmic variables.

+ [[https://ui.adsabs.harvard.edu/abs/2017MNRAS.466..163W/abstract][Wevers et al. (2017)]] paper present a catalog of H$\alpha$ emitters candidates.  
  
+ [[https://ui.adsabs.harvard.edu/abs/2020A%26A...643A.122S/abstract][Škoda et al. (2020)]] using deep learning found new emission line objects.
 
* Looking for Halpha emitters using color criteria and database
By inspecting the position of Splus synthetic photometry known emission line objects
in the (r-Ha)vs(r-i) color color diagram. I have divided the location of them in two zone.
Zone 2 is the region with high probability of being real emission line objects (ELO)
there are not contamination by main-sequence, and giant stars and WD (see, file:Fig-SPLUS-viironen.pdf).
Zone are populated by main-sequence, DWs and ELOs. These last have probably very small H\alpha excess.

*** Possible methods to find for ELO
For the zone 2, I think is just necessary apply a color criterion based in the position of the
known ELOs.
The zone 1: Here is most complicated due to the presence of MS and WD stars. 
Witham carry out an initial straight line least-squares fit toall objects in each magnitude bin.
I propose to use unsupervised machine learning to do that.

** Change
I decided to apply the criteria all data. I selected objects with very good photometry: errors minors and equal than 0.2 in all bands, flats in J0660, r and i  minor and equal than 2.0. 
At the moment I apply FWHM <= 2.0.

**** DBSCAN

The DBSCAN approach works on a very similar principle as grid-based methods. However,
unlike grid-based methods, the density characteristics of data points are used to merge them
into clusters. Therefore, the individual data points in dense regions are used as building
blocks after classifying them on the basis of their density.
The density of a data point is defined by the number of points that lie within a radius
Eps of that point (including the point itself). The densities of these spherical regions are
used to classify the data points into core, border, or noise points. These notions are defined
as follows:

1. Core point: A data point is defined as a core point, if it contains 4 at least τ data points.

2. Border point: A data point is defined as a border point, if it contains less than τ points,
   but it also contains at least one core point within a radius Eps.

***** DBSCAN with Python Scikit-Learn

Some examples using DBSCAN algorithms:
 + [[file:Notebook/DBSCAN testing.ipynb][DBSCAN]]
 + [[file:Notebook/K-distance graph to find the value of epsilon and other clustering algorithms .ipynb]] 


***** DBSCAN Parameter Estimation 

Source: [[https://medium.com/@tarammullin/dbscan-parameter-estimation-ff8330e3a3bd]]
Focus on estimating DBSCAN’s two parameters:

1. Minimum samples (“MinPts”): the fewest number of points required to form a cluster
2. \epsilon (epsilon or “eps”): the maximum distance two points can be from one another while still belonging to the same cluster

+ Minimum Samples (“MinPts”)
  There is no automatic way to determine the MinPts value for DBSCAN. Ultimately, the MinPts value should be set using domain knowledge and familiarity with the data set. From some research I’ve done, 
  here are a few rules of thumb for selecting the MinPts value:

    + The larger the data set, the larger the value of MinPts should be
    + If the data set is noisier, choose a larger value of MinPts
    + Generally, MinPts should be greater than or equal to the dimensionality of the data set
    + For 2-dimensional data, use DBSCAN’s default value of MinPts = 4 (Ester et al., 1996).
    + If your data has more than 2 dimensions, choose MinPts = 2*dim, where dim = the dimensions of your data set (Sander et al., 1998).

+ Epsilon (\epsilon)
  After you select your MinPts value, you can move on to determining ε. One technique to automatically determine the optimal \epsilon value 
  is described here. This technique calculates the average distance between each point and its k-nearest neighbors, 
  where k = the MinPts value you selected. 
  The average k-distances are then plotted in ascending order on a k-distance graph. 
  You’ll find the optimal value for \epsilon at the point of maximum curvature (i.e. where the graph has the greatest slope).


**** OPTICS

Put information here...

Some sources:
 + https://www.machinecurve.com/index.php/2020/12/15/performing-optics-clustering-with-python-and-scikit-learn/

... For this reason, OPTICS is preferable over DBSCAN when your clusters have varying density. In other cases, the choice for algorithm does not really matter.

***** OPTICS with Python Scikit-Learn

Some examples using OPTICS algorithms:
 + [[OPTICS Demo.ipynb]]
 + 
 
**** Some conclusions

I tested with JPLUS data, consult:
   [[file:../../JPLUS/emission_objects.org]]

I tried unsupervised machine learning to find the locus of Main sequence and Giant stars: 
+ I first try DBSCAN but no worked very well due to it falls with varying density. Many mini cluster were found and this dint make sense.
+ I tried OPTICS algorithm to find the MS and Giant locus. It works better than DBSCAN when we have data with varying density and don't need 
  to introduce the \epsilon parameter. The problem is take long take executing.  
+ Finally I try we HDBSCAN works very well. I made several test to find the best parameters that allow to better clustering.  

**** HDBSCAN

***** Test different parameters

#+BEGIN_SRC test hdbscan
+ min_samples=20, min_cluster_size=60 -> Estimated number of clusters: 4
                                         Estimated number of cluster points 0: 85
                                         Estimated number of cluster points 1: 129
                                         Estimated number of cluster points 2: 1167500
                                         Estimated number of noise points: 37670

+ min_samples=20, min_cluster_size=70 -> Estimated number of clusters: 4
                                         Estimated number of cluster points 0: 85
                                         Estimated number of cluster points 1: 129
                                         Estimated number of cluster points 2: 1167500
                                         Estimated number of noise points: 37670

+ min_samples=20, min_cluster_size=80 -> Estimated number of clusters: 3
                                         Estimated number of cluster points 0: 85
                                         Estimated number of cluster points 1: 129
                                         Estimated number of cluster points 2: 1190902
                                         Estimated number of noise points: 14344

+ min_samples=20, min_cluster_size=90 -> Estimated number of clusters: 2
                                         Estimated number of cluster points 0: 129
                                         Estimated number of cluster points 1: 1190902
                                         Estimated number of cluster points 2: 0
                                         Estimated number of noise points: 14429

+ min_samples=20, min_cluster_size=100 -> Same than min_samples=20, min_cluster_size=90
#+END_SRC

#+BEGIN_SRC test :hdbscan
+ min_samples=30, min_cluster_size=90 -> Cluster 0 172
                                         Cluster 1 1191735
                                         Cluster 2 0
                                         Noise 13553

+ min_samples=30, min_cluster_size=70 -> Estimated number of clusters: 3
                                         Estimated number of cluster points 0: 172
                                         Estimated number of cluster points 1: 78
                                         Estimated number of cluster points 2: 1171175
                                         Estimated number of noise points: 34035

+ min_samples=30, min_cluster_size=80 -> Estimated number of clusters: 2
                                         Estimated number of cluster points 0: 172
                                         Estimated number of cluster points 1: 1191735
                                         Estimated number of cluster points 2: 0
                                         Estimated number of noise points: 13553

+ min_samples=30, min_cluster_size=100 -> Estimated number of clusters: 2
                                          Estimated number of cluster points 0: 172
                                          Estimated number of cluster points 1: 1191735
                                          Estimated number of cluster points 2: 0
                                          Estimated number of noise points: 13553

+ min_samples=30, min_cluster_size=60 -> Estimated number of clusters: 3
                                         Estimated number of cluster points 0: 172
                                         Estimated number of cluster points 1: 78
                                         Estimated number of cluster points 2: 1171175
                                         Estimated number of noise points: 34035
#+END_SRC

#+BEGIN_SRC test :hdbscan
+ min_samples=40, min_cluster_size=60 -> Estimated number of clusters: 2
                                         Estimated number of cluster points 0: 122
                                         Estimated number of cluster points 1: 1191359
                                         Estimated number of cluster points 2: 0
                                         Estimated number of noise points: 13979

+ min_samples=40, min_cluster_size=70 -> Estimated number of clusters: 2
                                         Estimated number of cluster points 0: 122
                                         Estimated number of cluster points 1: 1191359
                                         Estimated number of cluster points 2: 0
                                         Estimated number of noise points: 13979

+ min_samples=40, min_cluster_size=80 -> Estimated number of clusters: 2
                                         Estimated number of cluster points 0: 122
                                         Estimated number of cluster points 1: 1191359
                                         Estimated number of cluster points 2: 0
                                         Estimated number of noise points: 13979  

+ min_samples=40, min_cluster_size=90 -> Same
#+END_SRC
-------------------------------------------------------------------------------------

#+BEGIN_SRC test hdbscan
+ min_samples=60, min_cluster_size=100 -> Estimated number of clusters: 4
                                          Estimated number of cluster points 0: 204
                                          Estimated number of cluster points 1: 112
                                          Estimated number of cluster points 2: 263
                                          Estimated number of noise points: 1153528
#+END_SRC
*** How download the data from the SPLUS database? 

IDR3 has 59,738,355 objects.

In agreement with the synthetic photometry, I downloaded the objects from the database with subjective equation: =r - H\alpha > 0.15*(r - i) - 0.4=.

#+BEGIN_SRC sql :query to select objects
SELECT detection.ID
FROM idr3.detection_image as detection JOIN idr3.u_band as u ON detection.ID=u.ID 
JOIN idr3.f378_band as f378 ON detection.ID=f378.ID JOIN idr3.f395_band as f395 ON detection.ID=f395.ID
JOIN idr3.f410_band as f410 ON detection.ID=f410.ID JOIN idr3.f430_band as f430 ON detection.ID=f430.ID JOIN idr3.g_band as g ON detection.ID=g.ID 
JOIN idr3.f515_band as f515 ON detection.ID=f515.ID JOIN idr3.r_band as r ON detection.ID=r.ID JOIN idr3.f660_band as f660 ON detection.ID=f660.ID 
JOIN idr3.i_band as i ON detection.ID=i.ID JOIN idr3.f861_band as f861 ON detection.ID=f861.ID JOIN idr3.z_band as z ON detection.ID=z.ID 
WHERE R_PStotal <= 21 AND F660_PStotal <= 21 AND I_PStotal <= 21 AND e_U_PStotal <= 0.2 AND e_F378_PStotal <= 0.2 AND e_F395_PStotal <= 0.2 AND e_F410_PStotal <= 0.2 
AND e_F430_PStotal <= 0.2 AND e_G_PStotal <= 0.2 AND e_F515_PStotal <= 0.2 AND e_R_PStotal <= 0.2 AND e_F660_PStotal <= 0.2 AND e_I_PStotal <= 0.2
AND e_F861_PStotal <= 0.2 AND e_Z_PStotal <= 0.2 
AND FWHM < 7.0 AND (R_PStotal - F660_PStotal) >= 0.15*(R_PStotal - I_PStotal) - 0.4
#+END_SRC


The above query take long time. Take 21min.
The next step is to make matching this Votable using website and ADQL language including the columns desire.

#+BEGIN_SRC sql :query to match with columns desire
SELECT detection.Field, detection.ID, detection.RA, detection.DEC, detection.FWHM, detection.ISOarea, detection.KRON_RADIUS, 
detection.nDet_magPStotal, detection.PhotoFlagDet, u.U_PStotal, f378.F378_PStotal, f395.F395_PStotal,
f410.F410_PStotal, f430.F430_PStotal, g.G_PStotal, f515.F515_PStotal, r.R_PStotal, f660.F660_PStotal, i.I_PStotal, 
f861.F861_PStotal, z.Z_PStotal, u.e_U_PStotal, f378.e_F378_PStotal, f395.e_F395_PStotal, f410.e_F410_PStotal, f430.e_F430_PStotal, 
g.e_G_PStotal, f515.e_F515_PStotal, r.e_R_PStotal, f660.e_F660_PStotal, i.e_I_PStotal, f861.e_F861_PStotal, z.e_Z_PStotal 
FROM TAP_UPLOAD.upload as tap JOIN idr3.detection_image as detection ON tap.ID= detection.ID  JOIN idr3.u_band as u ON tap.ID=u.ID 
JOIN idr3.f378_band as f378 ON tap.ID=f378.ID JOIN idr3.f395_band as f395 ON tap.ID=f395.ID
JOIN idr3.f410_band as f410 ON tap.ID=f410.ID JOIN idr3.f430_band as f430 ON tap.ID=f430.ID JOIN idr3.g_band as g ON tap.ID=g.ID 
JOIN idr3.f515_band as f515 ON tap.ID=f515.ID JOIN idr3.r_band as r ON tap.ID=r.ID JOIN idr3.f660_band as f660 ON tap.ID=f660.ID 
JOIN idr3.i_band as i ON tap.ID=i.ID JOIN idr3.f861_band as f861 ON tap.ID=f861.ID JOIN idr3.z_band as z ON tap.ID=z.ID 
#+END_SRC

I got the error:
: Error message: Error while reading the VOTable "upload": Data read overflow: the limit of 2000 rows has been reached!

Try the initial query include the columns desire.

#+BEGIN_SRC sql 
SELECT detection.Field, detection.ID, detection.RA, detection.DEC, detection.FWHM, detection.ISOarea, detection.KRON_RADIUS, 
detection.nDet_magPStotal, detection.PhotoFlagDet, u.U_PStotal, f378.F378_PStotal, f395.F395_PStotal,
f410.F410_PStotal, f430.F430_PStotal, g.G_PStotal, f515.F515_PStotal, r.R_PStotal, f660.F660_PStotal, i.I_PStotal, 
f861.F861_PStotal, z.Z_PStotal, u.e_U_PStotal, f378.e_F378_PStotal, f395.e_F395_PStotal, f410.e_F410_PStotal, f430.e_F430_PStotal, 
g.e_G_PStotal, f515.e_F515_PStotal, r.e_R_PStotal, f660.e_F660_PStotal, i.e_I_PStotal, f861.e_F861_PStotal, z.e_Z_PStotal
FROM idr3.detection_image as detection JOIN idr3.u_band as u ON detection.ID=u.ID 
JOIN idr3.f378_band as f378 ON detection.ID=f378.ID JOIN idr3.f395_band as f395 ON detection.ID=f395.ID
JOIN idr3.f410_band as f410 ON detection.ID=f410.ID JOIN idr3.f430_band as f430 ON detection.ID=f430.ID JOIN idr3.g_band as g ON detection.ID=g.ID 
JOIN idr3.f515_band as f515 ON detection.ID=f515.ID JOIN idr3.r_band as r ON detection.ID=r.ID JOIN idr3.f660_band as f660 ON detection.ID=f660.ID 
JOIN idr3.i_band as i ON detection.ID=i.ID JOIN idr3.f861_band as f861 ON detection.ID=f861.ID JOIN idr3.z_band as z ON detection.ID=z.ID 
WHERE R_PStotal <= 21 AND F660_PStotal <= 21 AND I_PStotal <= 21 AND e_U_PStotal <= 0.2 AND e_F378_PStotal <= 0.2 AND e_F395_PStotal <= 0.2 AND e_F410_PStotal <= 0.2 
AND e_F430_PStotal <= 0.2 AND e_G_PStotal <= 0.2 AND e_F515_PStotal <= 0.2 AND e_R_PStotal <= 0.2 AND e_F660_PStotal <= 0.2 AND e_I_PStotal <= 0.2
AND e_F861_PStotal <= 0.2 AND e_Z_PStotal <= 0.2 
AND FWHM < 7.0 AND (R_PStotal - F660_PStotal) >= 0.15*(R_PStotal - I_PStotal) - 0.4
#+END_SRC

Take 11mins.

I ran the query:

SELECT count(*) FROM idr3.detection_image as detection JOIN idr3.r_band as r ON detection.ID=r.ID JOIN idr3.f660_band as 
f660 ON detection.ID=f660.ID JOIN idr3.i_band as i ON detection.ID=i.ID 
WHERE R_PStotal <= 21 AND e_R_PStotal <= 0.2 
AND e_F660_PStotal <= 0.2 AND e_I_PStotal <= 0.2 AND FWHM < 7.0 AND R_PStotal < 16.0

Take around 45min.
Results =1251923=.

I found that using the Gustavos database is restricted to get table with 20000 rows.

**** Download stamps
Script to download stamp images FITS from database
: python ../../programs/get-splus-fits.py 351.0847562454346 -0.10695578184817373 --radi 300 --band F515 --name GALEX24170

**** Download colored and cut images from database

I write a script to download the colored images and make them for publication:

#+begin_src python
 '''
 Scrit to download colored images from database
 '''
 # Import the necessary packages 
 import splusdata 
 import pandas as pd
 import matplotlib.pyplot as plt
 import aplpy
 from astropy.io import fits
 from astropy.wcs import WCS
 import os
 import argparse
 import sys

 parser = argparse.ArgumentParser(
     description="""Get colored image and cut image in the r-band""")

 parser.add_argument("ra", type=float,
                     default="316.473196",
                     help="RA of the object")

 parser.add_argument("dec", type=float,
                    default="-37.144562",
                    help="Dec of the object")

 parser.add_argument("--radi", type=float, default=None,
                    help="""Size of the images in pixel""")

 parser.add_argument("--name", type=str, default=None,
                    help="""Name of the object""")

 cmd_args = parser.parse_args()
 ra = cmd_args.ra
 dec = cmd_args.dec

 # Radius
 rad = int(cmd_args.radi)

 # Nome of the object if has
 Name = cmd_args.name

 # Connect
 conn = splusdata.connect('Luis', 'plutarco*80')

 # Getting the colored imge
 img = conn.twelve_band_img(ra, dec, radius=rad, noise=0.15, saturation=0.15)

 # Getting the Fits image in the r-band
 hdu = conn.get_cut(ra, dec, rad, 'R')

 # Save the image, note that the output image in compress
 hdu.writeto('{}_{}-{}_{}_r.fz'.format(Name, ra, dec, rad), overwrite=True) # write to fits

 ############################################################
 # Definition to decompress the images ######################
 ############################################################
 def fz2fits(image):
     """
     It converts SPLUS images
     from .fz to .fits
     """
     datos = fits.open(image)[1].data
     heada = fits.open(image)[1].header
     imageout = image[:-2] + 'fits'
     print ('Creating file: ')
     print (imageout)
     fits.writeto(imageout, datos, heada, overwrite=True)
 ############################################################
 # Decompress
 hdufits = fz2fits('{}_{}-{}_{}_r.fz'.format(Name, ra, dec, rad))

 # Read the FITS file
 hdul = fits.open('{}_{}-{}_{}_r.fits'.format(Name, ra, dec, rad))[0]
 wcs = WCS(hdul.header)

 print(wcs)                 

 f = plt.figure(figsize=(18,9))

 ax1 = aplpy.FITSFigure(hdul, figure=f, subplot=(1, 1, 1))#, north=True)
 plt.imshow(img, origin='lower', cmap='cividis', aspect='equal')
                 
 ax1.add_scalebar(20.0/3600)
 ax1.scalebar.set_label('20 arcsec')
 ax1.scalebar.set(color='yellow', linewidth=4, alpha=0.9)
 ax1.scalebar.set_font(size=23, weight='bold',
                      stretch='normal', family='sans-serif',
                      style='normal', variant='normal')

 ax1.axis_labels.set_font(size=22, weight='medium', stretch='normal', family='sans-serif', style='normal', variant='normal')
 #img.axis_labels.hide()
 #img.axis_labels.hide_y()

 ax1.tick_labels.set_font(size=22, weight='medium', stretch='normal', family='sans-serif', style='normal', variant='normal')
 #ax1.list_layers()
 #ax1.show_markers(ra, dec, layer='marker', edgecolor='green', facecolor='none', marker='o', s=10, alpha=0.9, linewidths=60)#, layer='marker_set_1', edgecolor='black', facecolor='none', s=30, alpha=0.5, linewidths=20)


 # ax1.axis_labels.hide_y()
 # ax1.tick_labels.hide_y()

 #ax2.colorbar.set_box([0.95, 0.1, 0.015, 0.8])
 ax1.set_theme('publication')
 #f.tight_layout()
 #f.savefig("-".join([image_name, "images.pdf"]))

 plt.savefig('{}_{}-{}_{}_r.fits'.format(Name, ra, dec, rad).replace(".fits", ".pdf"))
#+END_SRC

: python ../../programs/getColored-imgesSplus.py 338.66637521384126 0.6909664418071105 --radi 100 --name FASTT1560

***** Or make 3-band colored images
: python ../../programs/rgb_image-v2.py GALEX24170_351-0_300_F515 GALEX24170_351-0_300_F660 GALEX24170_351-0_300_F515 --debug



*** Alternative solution: sqlite

A possible solution is to create a database in my own machine.
Disadvantage: The file are very large. For instance HYDRA catalog has ~15G in size.

Considering https://github.com/astropy/astropy/pull/4760 for sigma clipping.

*** Change the methodology

Find the locus of MS stars by line fit to all data and apply the methodology from Witham et al. (2006) to select the H\alpha emitters. After apply HDBSCAN to find the locus of MS stars to compare 
with the other one.   

+ I created my own data base with DR3. After I wrote a script on python to download the data with these criteria: Flags_allFilter <= 2, error_allFilter <= 0.2, and magnitude interval on r-band.
: python apply_query.py

*** Final methods: Linear fit
Method based on Witham et al. (2008) and Wevers et al. (2017).
: python ../MC/programs/Selecting_Halpha_objects.py LMC_catalog_3fer_18r20 --Ranger "18 $\geq$ r < 20"

**** Second phase: Cross-match 
+ Lamost -> 
  I mede macth with dr6.


+ Match with sloan. Limited to 1000 rows.
  Program to write the table in SDSS format to download spectra and to split the table:
: python ../programs/coordinate_forSloantMacth.py Halpha-DR3_noFlag_merge

+ Match with [[http://skyserver.sdss.org/dr16/en/tools/search/SQS.aspx][SDSS]]

**** S-spectra
- I used the r-band  and 6250.289 Angstrom on Lamost. 

- Program:
: python ../../programs/splus_sdss_spectra.py spec-9152-58041-0463 Halpha-DR3_noFlag_3ferr_merge --ymin -0.05 --ymax 0.62
: for f in *.fits; do python ../../programs/splus_sdss_spectra.py ${f%.fits} Halpha-DR3_noFlag_3ferr_merge --ymax 3; done

**** Several plots

Plots with the results

: python ../programs/results.py 

+ What about try a plot r-mag vs b(Gal)?

*** Clean the final tables using ML

** Linear dicriminad analysis

Using linear discriminant analisys to clean the final sample of H{$\alpha$} emitters:

#+BEGIN_SRC results

 Shape of array: (419, 12)
 Accuracy score for Testing Dataset =  0.8333333333333334
 Precision score for Testing Dataset =  0.8205128205128205
 Confusion matrix =  (38  7)
                    ( 7 32)
 Data to classify: (13644, 12)
 13644

#+END_SRC


#+BEGIN_SRC results

Shape of array: (419, 7)
Accuracy score for Testing Dataset =  0.7976190476190477
Precision score for Testing Dataset =  0.7948717948717948
Confusion matrix =  (36  9)
                    (8 31)
Data to classify: (13644, 7)
13644
#+END_SRC

** Things that remain to be done 
+ Finished the red and blue classification: 
  - Compare the synthetic color with the observed one.
  - Compare with statistical techniques (Hierarchical models).
:DONE:

+ Used the QSOs, stars and galaxy classification.
  - Tutorial of Liliene about how implement the star, galaxy and QSO classification:

#+BEGIN_SRC  e-mail
    1. Baixar última versão do splusdata (3.65)
       pip install --upgrade splusdata

    2. No Python:
       from splusdata.features.g_star import ClassifyObj #pode acabar mudando em breve essa parte do "splusdata.features.g_star"
       clf = ClassifyObj(data, model="RF16", return_prob = True, match_irsa=False)
       clf.results #acessa os resultados. Os índices das linhas têm direta correspondência com os índices do dataframe de input. 

       ps: vai mudar todo o fluxo em breve na proxima versão do splusdata
       ----------------------

       data: pandas dataframe que precisa obrigatoriamente ter as seguintes informações

       ['FWHM_n', 'A', 'B', 'KRON_RADIUS', 'u_iso', 'J0378_iso',
        'J0395_iso','J0410_iso','J0430_iso', 'g_iso','J0515_iso',
        'r_iso','J0660_iso', 'i_iso','J0861_iso', 'z_iso']

       as magnitudes precisam estar previamente corrigidas pela extinção

       model: ["RF16", "RF18" ou "both"] determina com qual modelo vai ser gerada as classificações.
       "RF16": usa o modelo que leva em consideração apenas os dados do S-PLUS
       "RF18": usa o modelo que leva em consideração os dados do S-PLUS e W1 e W2 do WISE (em magnitude vega) 
       "both": se usar essa opção vai retornar a classificação RF18 para objetos que tem informação no WISE (model_flag == 0), caso contrário vai retornar a classificação por RF16 (model_flag==1). 
       Nota: "RF18" pode retornar um dataframe MENOR que o dataframe de input pois nem todos os objetos terão informação no WISE. Os indíces da linha são mantidos, então se fizer um pd.concat([data, clf.results], axis=1) irá fazer o join das tabelas de forma correta. 

       return_prob: [True ou False] se True, determina se irá retornar as probabilidades. Caso contrário, retornará apenas a classe (0: QSO, 1: STAR, 2: GALAXY)

       match_irsa: [True ou False] se True, irá fazer query do ALLWISE catalogue e fará o crossmatch com os dados de entrada para que seja possível rodar o modelo RF18. Se model == "RF16", match_irsa == False automaticamente.

       verbose: [True ou False] se True, retorna prints de cada etapa
#+END_SRC
 
  - Cross-match to download the table with the right columns.

       Query:

#+BEGIN_SRC sql : query for match
       SELECT detection.Field, detection.ID, detection.RA, detection.DEC, detection.FWHM, detection.FWHM_n, detection.A, 
       detection.B, detection.ISOarea, detection.KRON_RADIUS, 
       detection.nDet_PStotal, detection.PhotoFlagDet, u.u_PStotal, J0378.J0378_PStotal, J0395.J0395_PStotal,
       J0410.J0410_PStotal, J0430.J0430_PStotal, g.g_PStotal, J0515.J0515_PStotal, r.r_PStotal, J0660.J0660_PStotal, i.i_PStotal, 
       J0861.J0861_PStotal, z.z_PStotal, u.e_u_PStotal, J0378.e_J0378_PStotal, J0395.e_J0395_PStotal, J0410.e_J0410_PStotal, J0430.e_J0430_PStotal, 
       g.e_g_PStotal, J0515.e_J0515_PStotal, r.e_r_PStotal, J0660.e_J0660_PStotal, i.e_i_PStotal, J0861.e_J0861_PStotal, z.e_z_PStotal, 
       u.u_iso, J0378.J0378_iso, J0395.J0395_iso,
       J0410.J0410_iso, J0430.J0430_iso, g.g_iso, J0515.J0515_iso, r.r_iso, J0660.J0660_iso, i.i_iso, 
       J0861.J0861_iso, z.z_iso, u.e_u_iso, J0378.e_J0378_iso, J0395.e_J0395_iso, J0410.e_J0410_iso, J0430.e_J0430_iso, 
       g.e_g_iso, J0515.e_J0515_iso, r.e_r_iso, J0660.e_J0660_iso, i.e_i_iso, J0861.e_J0861_iso, z.e_z_iso 
       FROM TAP_UPLOAD.upload as tap JOIN idr3.detection_image as detection ON tap.ID= detection.ID JOIN idr3.u_band as u ON tap.ID=u.ID 
       JOIN idr3.j0378_band as J0378 ON tap.ID=J0378.ID JOIN idr3.J0395_band as J0395 ON tap.ID=J0395.ID
       JOIN idr3.J0410_band as J0410 ON tap.ID=J0410.ID JOIN idr3.J0430_band as J0430 ON tap.ID=J0430.ID JOIN idr3.g_band as g ON tap.ID=g.ID 
       JOIN idr3.J0515_band as J0515 ON tap.ID=J0515.ID JOIN idr3.r_band as r ON tap.ID=r.ID JOIN idr3.J0660_band as J0660 ON tap.ID=J0660.ID 
       JOIN idr3.i_band as i ON tap.ID=i.ID JOIN idr3.J0861_band as J0861 ON tap.ID=J0861.ID JOIN idr3.z_band as z ON tap.ID=z.ID 
#+END_SRC
      
       Before to apply the query I split the file (I am not sure if necessary) but the splus documentation says that the cross match is restringing to 2000 rows.
       : python ../../programs/join-csvtable.py

+ Cross-match SDSS, LAMOST, SIMBAD.

+ The separation between blue and red sources sees works well.
+ I checked visually all the objects of my list of Halpha emitters.
+ I made a latex table with all objects crossmatched with Simbad. This table include the 
  classification red and blue sources  using the hierarchical cluster analysis and the
  probability of belong each class using HDBSCAN. 

** Notes  
+ The object iDR3.SPLUS-n05n50.017121 (216.6133870755976, 5.065007143584472) is interesant.
+ PN NGC 3132 iDR3.HYDRA-0121.061126 (151.7447846287751, -40.4433163332109)
+ Spectra for the paper:
  - STRIPE82-0026-058736 -> H II galaxy
  - STRIPE82-0031.029096 -> H II galaxy
  - STRIPE82-0027.022810 -> H II galaxy
  - STRIPE82-0108.039253 -> CV
  - STRIPE82-0142.027354 -> CV 
  - STRIPE82-0152.056707 -> CV
  - STRIPE82-0165.038101 -> CV
  - STRIPE82-0103.089600 -> CV
  - STRIPE82-0159.019049 -> H II regions
  - STRIPE82-0028.045006 (18.244350529013424 0.9761179117928264) -> extra H II region
  - STRIPE82-0026.037373 (16.580131544341818 0.8064950808416276) -> QSO
  - SPLUS-n03s23.033092 (180.0906960738449 -2.72525835885637) -> QSO, very weak, would be good idea put QSO with very weak line.
  - STRIPE82-0032.003020 -> QSO
  - STRIPE82-0044.021486 -> QSO
  - STRIPE82-0003.028018 -> QSO
  - STRIPE82-0143.016137 -> QSO, select
  - SPLUS-n02s23.042426 (180.1095757612321 -1.1019334954271696) -> WR galaxy
  - SPLUS-n03s28.019988 (187.95005920551247 -2.970279786813015) -> WR galaxy
  		
+ Final spectra of known objects for the paper at the moment:
  - PN G006.0-41.9 -> SPLUS-s29s46.072842, (316.4731956938531 -37.14456181858315).
  - H II GALEX 2417063145906373262 -> STRIPE82-0159.019049 (351.0847562454346 -0.10695578184817373)
  - CV FASTT 1560 -> STRIPE82-0142.027354 (338.66637521384126 0.6909664418071105)
  - HIIG LEDA 1185205 -> STRIPE82-0026.058736 (17.28310789223492 1.12097992550518)
  - QSO PHL   354 -> STRIPE82-0143.016137 (339.59688919210015 -0.952268013442764)

+ Three spectra from Lamost:
  - STRIPE82-0057.001810 (39.68852506534099 -1.3705561161546969) -> Interesting object could be a SySt.
  - SPLUS-s03s05.011958 (329.5210752335242 -3.1300673601973967) -> Looks like a galaxy.
  - STRIPE82-0084.014280 (57.74100567327152 0.4260298565588763) -> Very week object. It is probably a quasar.

+ Three spectra from SDSS:
  - STRIPE82-0007.024265 (3.6458604175197946 -0.731125956040772)
  - STRIPE82-0147.005730 (343.4952075319428 -1.289144725738761)
  - SPLUS-n02s23.034336 (180.29305577347304 -1.2960504262969996)

+ S-spectra in magnitude unity
  - iDR3.SPLUS-n15s22.026955 (180.482084992802 -18.87855472820429)
  - iDR3.SPLUS-n15s22.024043 (179.74240913634958 -19.02992833661004)
  - iDR3.SPLUS-s02s12.022219 (339.5969264365535 -0.9522370681869864)
  - DR3.SPLUS-n12s32.020869 (194.88643900048527 -15.23870530424191)
  - iDR3.HYDRA-0026.052331  (158.85933642579576 -24.753136157195524)
  - iDR3.SPLUS-n15s20.042930 (177.11120124325876 -18.50628940436583), PN?
*** Repeat objects
- iDR3.HYDRA-0012.045435
- iDR3.SPLUS-s24s57.021041
- iDR3.SPLUS-s24s57.030527
- iDR3.SPLUS-s27s12.001173
- iDR3.HYDRA-0012.030069
- iDR3.SPLUS-s27s24.009065
- iDR3.SPLUS-s27s24.040526
- iDR3.SPLUS-s24s57.031431
- iDR3.SPLUS-s27s24.035964
- iDR3.HYDRA-0012.025156
- iDR3.SPLUS-s27s12.021798
*** PNe
- iDR3.HYDRA-0145.120355
*** Possible new CV  
Found in the list of Halpha emitters. The S-spectra is very to those confirmed CVs.
- iDR3.SPLUS-n14s09.024396 (160.9775297560119 -17.75830850106541)
- iDR3.SPLUS-s24s41.036794 (62.896990490717535 -30.370728973400283)
- iDR3.HYDRA-0163.093333   (155.39924683710603	-47.316427665698775)
- iDR3.SPLUS-s27s34.027141 (53.50745961827551	-34.73551729784952)

*** Take in count
- SPLUS-n05n50,iDR3.SPLUS-n05n50.013917 (217.2307884592734,5.0060874486010265):
  Simbad Possible_lensImage or blue.


** SIMBAD
I made cross-match with SIMBAD.
So, working with the SIMBAD sample.

#+name: simbad-data
#+BEGIN_SRC python :tangle /programs/simbad-data.py :return filename :results file
  #from __future__ import print_function
  import numpy as np
  from sklearn import metrics
  from scipy.optimize import curve_fit
  import pandas as pd
  from astropy.table import Table
  import seaborn as sns
  from astropy.stats import sigma_clip
  import matplotlib.pyplot as plt
  from matplotlib.font_manager import FontProperties
  from sklearn.metrics import mean_squared_error
  from astropy.modeling import models, fitting
  import argparse
  import sys
  import os
  from pathlib import Path
  ROOT_PATH = Path("paper/Figs")
  
  df = pd.read_csv("iDR3_n4/simbad.csv")
  print(df.columns)
  
  # MASKs
  m1 = df['main_type'] == 'AGN_Candidate'
  m2 = df['main_type'] == 'AGN'
  m3 = df['main_type'] == 'EmG'
  m4 = df['main_type'] == 'GinGroup'
  m5 = df['main_type'] == 'Galaxy'
  m6 = df['main_type'] == 'Candidate_CV*'
  m7 = df['main_type'] == 'RRLyr'
  m8 = df['main_type'] == 'SN'
  m9 = df['main_type'] == 'HII'
  m11 = df['main_type'] == 'CataclyV*'
  m12 = df['main_type'] == 'FIR'
  m13 = df['main_type'] == 'GinCl'
  m14 = df['main_type'] == 'HII_G'
  m15 = df['main_type'] == 'Seyfert_1'
  m17 = df['main_type'] == 'Star'
  m18 = df['main_type'] == 'PartofG'
  m19 = df['main_type'] == 'RadioG'
  m20 = df['main_type'] == 'IG'
  m21 = df['main_type'] == 'QSO'
  m22 = df['main_type'] == 'EB*'
  m23 = df['main_type'] == 'Radio'
  m24 = df['main_type'] == 'Seyfert_2'
  m26 = df['main_type'] == 'X'
  m27 = df['main_type'] == 'MolCld'
  m28 = df['main_type'] == 'Cl*'
  m29 = df['main_type'] == 'HMXB'
  m30 = df['main_type'] == 'GinPair'
  m31 = df['main_type'] == 'LSB_G'
  m32 = df['main_type'] == 'WD*'
  m33 = df['main_type'] == 'Candidate_RRLyr'
  m34 = df['main_type'] == 'PN'
  m35 = df['main_type'] == 'Blue'
  m36 = df['main_type'] == 'EmObj'
  m37 = df['main_type'] == 'BlueSG*'
  m38 = df['main_type'] == 'StarburstG'
  m39 = df['main_type'] == 'low-mass*'
  m40 = df['main_type'] == 'BlueCompG'
  m41 = df['main_type'] == 'UV'
  m42 = df['main_type'] == 'Candidate_WD*'
  m43 = df['main_type'] == 'MIR'
  m44 = df['main_type'] == 'Radio(cm)'
  m45 = df['main_type'] == 'Candidate_SN*'
  m46 = df['main_type'] == 'QSO_Candidate'
  m47 = df['main_type'] == 'BLLac'
  m48 = df['main_type'] == 'PM*'
  m49 = df['main_type'] == 'Possible_lensImage'
  m51 = df['main_type'] == 'Nova'
  m52 = df['main_type'] == 'BClG'
  m53 = df['main_type'] == 'GlCl'
  
  # Making the tables with individual object classes
  df_agn = pd.concat([df[m1], df[m2]])
  df_EmG = pd.concat([df[m3], df[m14], df[m38], df[m40]])
  GinGroup = df[m4]
  df_pn = df[m34] 
  df_gal = df[m5]
  df_qso = pd.concat([df[m21], df[m46]])
  df_cv = pd.concat([df[m6], df[m11]])
  df_hii = df[m9]
  df_star = df[m17]
  df_sn = pd.concat([df[m8], df[m45]])
  df_rrly = pd.concat([df[m7], df[m33]])
  df_fir = df[m12]
  df_GinCl = df[m13]
  df_Seyfert_1 = df[m15]
  df_Seyfert_2 = df[m24]
  df_PartofG = df[m18]
  df_RadioG = df[m19]
  df_IG = df[m20]
  df_EB = df[m22]
  df_Radio = pd.concat([df[m23], df[m44]])
  df_X = df[m26]
  df_MolCld = df[m27]
  df_cl = df[m28]
  df_HMXB = df[m29]
  df_GinPair = df[m30]
  df_LSB_G = df[m31]
  df_WD = pd.concat([df[m32], df[m42]])
  df_Blue = df[m35]
  df_EmObj = df[m36]
  df_BlueSG = df[m37]
  df_low_mass = df[m39]
  df_uv = df[m41]
  df_mier = df[m43]
  df_BLLac = df[m47]
  df_pm = df[m48]
  df_Possible_lensImage = df[m49]
  df_nova = df[m51]
  df_bclg = df[m52]
  df_glcl = df[m53]
  
  # Countaining the sources of each class 
  list_numbers = ["H II regions", "PN", "CV", "SN", "Nova", "BL Lac",
		  "Variable Star of RR Lyr type",
		  "Star", "WD", "Cluster of Stars", "HMXB", "Far-Infrared source",
		  "Eclipsing binary", "Blue object", "Emission Object", "Blue supergiant star",
		  "Low-mass star", "UV-emission source", "MIER", "Possible lens Image"
		  "Galaxy", "Galaxy in Pair of Galaxies",
		  "Emission line galaxies",
		  "QSO", "AGN", "Part of a Galaxy", "X-ray source", "Molecular Cloud",
		  "Galaxy in Group of Galaxies", "Radio-source", "Interacting Galaxies",
		  "Low Surface Brightness Galaxy", "Radio Galaxy", "Galaxy in Cluster of Galaxies"
		  "High proper-motion Star", "Seyfert 1", "Seyfert 2",
		  "Brightest galaxy in a Cluster", "Globular Cluster"]
  
  # Definition to make the colors
  def colour(tab, f1, f2, f3, f4):
      xcolour = tab[f1] - tab[f2]
      ycolour = tab[f3] - tab[f4]
      return xcolour, ycolour
  
  # Colors
  cx_pn, cy_pn = colour(df_pn, "Z_PStotal", "G_PStotal", "G_PStotal", "R_PStotal")
  cx_gal, cy_gal = colour(df_gal, "Z_PStotal", "G_PStotal", "G_PStotal", "R_PStotal")
  cx_EmG, cy_EmG = colour(df_EmG, "Z_PStotal", "G_PStotal", "G_PStotal", "R_PStotal")
  cx_qso, cy_qso = colour(df_qso, "Z_PStotal", "G_PStotal", "G_PStotal", "R_PStotal")
  cx_cv, cy_cv = colour(df_cv, "Z_PStotal", "G_PStotal", "G_PStotal", "R_PStotal")
  cx_hii, cy_hii = colour(df_hii, "Z_PStotal", "G_PStotal", "G_PStotal", "R_PStotal")
  cx_star, cy_star = colour(df_star, "Z_PStotal", "G_PStotal", "G_PStotal", "R_PStotal")
  
  #PLOT
  # Limiting the blue and red region
  x_new = np.linspace(-15.0, 1000, 200)
  y = 0.45*x_new + 1.55
  
  fig, ax = plt.subplots(figsize=(12, 12))
  
  ax.fill_between(x_new, y, -100, color="k", alpha=0.1)
  ax.plot(x_new, y, c="k", zorder=11, lw=0.5)
  
  plt.tick_params(axis='x', labelsize=25) 
  plt.tick_params(axis='y', labelsize=25)
  
  plt.xlabel(r'$z - g$', fontsize= 25)
  plt.ylabel(r'$g - r$', fontsize= 25)
  
  ax.scatter(
	  cx_pn, cy_pn,
	  marker="o",
	  c=sns.xkcd_rgb["cerulean"],
	  label="PN",
	  edgecolors="w", alpha=0.7, zorder=4
      )
  
  ax.scatter(
	  cx_gal, cy_gal,
	  marker="o",
	  c=sns.xkcd_rgb["dark pink"],
	  label="Galaxy",
	  edgecolors="w", zorder=3
      )
  
  ax.scatter(
	  cx_EmG, cy_EmG,
	  marker="o",
	  c=sns.xkcd_rgb["bright blue"],
	  label="EmG",
	  edgecolors="w", zorder=3
      )
  
  ax.scatter(
	  cx_qso, cy_qso,
	  marker="o",
	  c=sns.xkcd_rgb["green"],
	  label="QSO",
	  edgecolors="w", zorder=3
      )
  
  ax.scatter(
	  cx_cv, cy_cv,
	  marker="o",
	  c=sns.xkcd_rgb["periwinkle"],
	  label="CV",
	  edgecolors="w", zorder=5
      )
  
  ax.scatter(
	  cx_hii, cy_hii,
	  marker="o",
	  c=sns.xkcd_rgb["pale yellow"],
	  label="HII Region",
	  edgecolors="w", zorder=4
      )
  
  ax.scatter(
	  cx_star, cy_star,
	  marker="o",
	  c=sns.xkcd_rgb["army green"],
	  label="Star",
	  edgecolors="w", zorder=6
      )
  
  
  ax.legend(ncol=1, fontsize=20.0, title_fontsize=30)
  ax.set(xlim=[-6.8, 2.5], ylim=[-3., 5.])#, xscale="log", yscale="log")
  ax.set_aspect("equal")
  #ax.set(xlabel=r"$z - g$", ylabel=r"$g - r$")
  
  filefile ="colour-digram-simbadObj.pdf"
  fig.savefig(ROOT_PATH / filefile)
  
  # Saving subtables (QSO)
  print(df_qso)
  filename = "iDR3_n4/QSO-simabad.ecsv"
  Table.from_pandas(df_qso).write(filename, format="ascii.ecsv")
  
  
#+END_SRC

#+RESULTS: simbad-data
[[file:iDR3_n4/QSO-simabad.ecsv]]
[[file:Index(['Field', 'ID', 'RA', 'DEC', 'FWHM', 'FWHM_n', 'A', 'B', 'ISOarea',
       'KRON_RADIUS', 'PhotoFlagDet', 's2n_Det_iso', 'U_PStotal',
       'F378_PStotal', 'F395_PStotal', 'F410_PStotal', 'F430_PStotal',
       'G_PStotal', 'F515_PStotal', 'R_PStotal', 'F660_PStotal', 'I_PStotal',
       'F861_PStotal', 'Z_PStotal', 'e_U_PStotal', 'e_F378_PStotal',
       'e_F395_PStotal', 'e_F410_PStotal', 'e_F430_PStotal', 'e_G_PStotal',
       'e_F515_PStotal', 'e_R_PStotal', 'e_F660_PStotal', 'e_I_PStotal',
       'e_F861_PStotal', 'e_Z_PStotal', 'F378_iso', 'F395_iso', 'F410_iso',
       'F430_iso', 'G_iso', 'F515_iso', 'R_iso', 'F660_iso', 'I_iso',
       'F861_iso', 'Z_iso', 'e_U_iso', 'e_F378_iso', 'e_F395_iso',
       'e_F410_iso', 'e_F430_iso', 'e_G_iso', 'e_F515_iso', 'e_R_iso',
       'e_F660_iso', 'e_I_iso', 'e_F861_iso', 'e_Z_iso', 'r - i', 'r - J0660',
       'e(r - i)', 'e(r - J0660)', 'P(GoodPho)', 'P(BadPho)', 'main_id',
       'ra_x', 'dec_x', 'coo_err_maj', 'coo_err_min', 'coo_err_angle', 'nbref',
       'ra_sexa', 'dec_sexa', 'main_type', 'other_types', 'radvel', 'redshift',
       'sp_type', 'morph_type', 'plx', 'pmra', 'pmdec', 'size_maj', 'size_min',
       'size_angle', 'B_x', 'V', 'R_x', 'J', 'H', 'K', 'u', 'g', 'r_xa', 'i',
       'z', 'angDist'],
      dtype='object')
              Field                         ID  ...       z   angDist
39     SPLUS-n12s01   iDR3.SPLUS-n12s01.020886  ...     NaN  0.169442
52    STRIPE82-0004  iDR3.STRIPE82-0004.031191  ...  16.267  0.145873
55    STRIPE82-0104  iDR3.STRIPE82-0104.027921  ...  16.986  0.103838
65     SPLUS-n09s43   iDR3.SPLUS-n09s43.035461  ...     NaN  0.147378
66     SPLUS-n10s36   iDR3.SPLUS-n10s36.018587  ...     NaN  0.539756
...             ...                        ...  ...     ...       ...
1045   SPLUS-n02n27   iDR3.SPLUS-n02n27.022071  ...  20.729  0.040387
1046   SPLUS-n02s19   iDR3.SPLUS-n02s19.000577  ...  20.790  0.266297
1048   SPLUS-n02s37   iDR3.SPLUS-n02s37.051418  ...  20.486  0.136084
1049   SPLUS-s35s45   iDR3.SPLUS-s35s45.031885  ...     NaN  1.500802
787   STRIPE82-0074  iDR3.STRIPE82-0074.040778  ...  19.410  0.102930

[225 rows x 98 columns]
]]

*** QSOs

Trying to get the Redshift of the QSOs from simbad.

#+name: QSO-simbad
#+BEGIN_SRC python :tangle /programs/simbad-QSO.py :return asciifile :results file
  from astroquery.simbad import Simbad
  import astropy.coordinates as coord
  import astropy.units as u
  from astropy.table import Table, hstack
  from astropy.coordinates import SkyCoord 
  import numpy as np
  from scipy.constants import constants
  
  # Read the table
  tab = Table.read("iDR3_n4/QSO-simabad.ecsv", format="ascii.ecsv")
  #Looking the objects of the table in Simbad and specifying the column wanted of Simbad
  customSimbad = Simbad()
  customSimbad.get_votable_fields()
  #customSimbad.list_votable_fields()
  customSimbad.add_votable_fields('sptype', 'otype','distance', "velocity")
  customSimbad.add_votable_fields("uvby")
  customSimbad.add_votable_fields('ra(d)', 'dec(d)')
  customSimbad.add_votable_fields('bibcodelist(1990-2020)')#, 'biblio')
  ## get redshift of object
  customSimbad.add_votable_fields("rv_value") 
  result_table = customSimbad.query_region(coord.SkyCoord(tab["RA"], tab["DEC"], 
						 unit=(u.deg, u.deg)), radius = 2.0 * u.arcsec)
  
  #Calculating redshift
  rv = result_table["RV_VALUE"].quantity.data[0]*1000 # rv in m / s
  z = rv / constants.c
  
  result_table["Redshift"] = z
  
  #save the table resulting
  asciifile = "iDR3_n4/QSO-simabad-redshift.ecsv"
  result_table.write(asciifile, format="ascii.ecsv")  
  
#+END_SRC

#+RESULTS: QSO-simbad
[[file:iDR3_n4/QSO-simabad-redshift.ecsv]]

#+BEGIN_SRC sh
  pip install constants
#+END_SRC

#+RESULTS:
| Collecting   | constants              |                 |                                                                                        |                                           |           |                                                                         |        |        |
| Downloading  | constants-0.6.0.tar.gz | (5.1            | kB)                                                                                    |                                           |           |                                                                         |        |        |
| Building     | wheels                 | for             | collected                                                                              | packages:                                 | constants |                                                                         |        |        |
| Building     | wheel                  | for             | constants                                                                              | (setup.py):                               | started   |                                                                         |        |        |
| Building     | wheel                  | for             | constants                                                                              | (setup.py):                               | finished  | with                                                                    | status | 'done' |
| Created      | wheel                  | for             | constants:                                                                             | filename=constants-0.6.0-py3-none-any.whl | size=5458 | sha256=a711a23bba1fafd7bbb2e4bc83d9b97b11370a83fd5ce9bef79882b9dca9b262 |        |        |
| Stored       | in                     | directory:      | /home/luis/.cache/pip/wheels/77/ac/b2/89268490b92bf6fd0102b3634668042437e0e024c64ef447a1 |                                           |           |                                                                         |        |        |
| Successfully | built                  | constants       |                                                                                        |                                           |           |                                                                         |        |        |
| Installing   | collected              | packages:       | constants                                                                              |                                           |           |                                                                         |        |        |
| Successfully | installed              | constants-0.6.0 |                                                                                        |                                           |           |                                                                         |        |        |

Now we need to do the table latex with the Redshift

#+BEGIN_SRC python :result output
  from astropy.table import Table
  
  
  
  
#+END_SRC

#+RESULTS:
: None
